"""Exploitability calculation for game theory equilibrium evaluation.

Exploitability measures how far a strategy profile is from Nash equilibrium by
computing the IMPERFECT INFORMATION best response against it.

Key insight: The best responder only knows their own card, not the opponent's.
This is computed by averaging over all possible opponent cards weighted by
the opponent's strategy.
"""

from typing import Protocol
import numpy as np
import numpy.typing as npt


class GameState(Protocol):
    """Protocol for game states."""

    def apply_action(self, action: int) -> "GameState": ...
    def legal_actions(self) -> list[int]: ...
    def is_terminal(self) -> bool: ...
    def returns(self) -> tuple[float, float]: ...
    def current_player(self) -> int: ...
    def information_state_string(self) -> str: ...
    def is_chance_node(self) -> bool: ...
    def chance_outcomes(self) -> list[tuple[int, float]]: ...


def best_response_value(
    state: GameState,
    best_responder: int,
    opponent_strategy: dict[str, npt.NDArray[np.float64]],
    fallback_strategy: dict[str, npt.NDArray[np.float64]] | None = None,
) -> float:
    """Compute IMPERFECT INFORMATION best response value.

    CRITICAL FIX: For imperfect information games, we must compute the best action
    AT THE INFORMATION SET LEVEL, not at individual world states. This means:
    1. Accumulate counterfactual values for each (info_set, action) pair
    2. For each info set, choose the action with highest average value
    3. Return the value of playing that best response strategy

    The counterfactual value weights by opponent reach probability, not player reach.

    Args:
        state: Current game state
        best_responder: Player computing best response (0 or 1)
        opponent_strategy: Opponent's strategy (info_state -> probabilities)
        fallback_strategy: Optional fallback strategy for tie-breaking

    Returns:
        Expected value for best_responder when playing optimally
    """
    # First pass: accumulate counterfactual values for each (infoset, action) pair
    infoset_values = {}
    _accumulate_infoset_values(state, best_responder, opponent_strategy, 1.0, 1.0, infoset_values)

    # Second pass: evaluate best response strategy
    best_response_strategy = {}
    for info_state, info_data in infoset_values.items():
        action_values = info_data["values"]
        total_reach = info_data["reach"]

        # Normalize values by total reach (handles multiple visits from different worlds)
        if total_reach > 1e-10:
            normalized_values = {a: v / total_reach for a, v in action_values.items()}
        else:
            # No reach to this info set (shouldn't happen in valid games)
            normalized_values = action_values

        # Pick action with highest normalized value
        if len(normalized_values) == 0:
            # No actions (shouldn't happen)
            best_action = 0
        else:
            # Check if there are meaningful differences
            values = list(normalized_values.values())
            max_val = max(values)
            min_val = min(values)

            # If all values are very close (especially if all zero), use fallback strategy
            if (
                abs(max_val - min_val) < 1e-10
                and fallback_strategy
                and info_state in fallback_strategy
            ):
                # Use fallback strategy (uniformly random best action chosen from fallback)
                fallback = fallback_strategy[info_state]
                best_action = max(normalized_values.keys(), key=lambda a: fallback[a])
            else:
                best_action = max(normalized_values.keys(), key=lambda a: normalized_values[a])

        # Create deterministic best response strategy
        num_actions = len(normalized_values)
        strategy = np.zeros(num_actions)
        strategy[best_action] = 1.0
        best_response_strategy[info_state] = strategy

    # Evaluate the best response strategy
    return _evaluate_strategy(state, best_responder, best_response_strategy, opponent_strategy)


def _accumulate_infoset_values(
    state: GameState,
    best_responder: int,
    opponent_strategy: dict[str, npt.NDArray[np.float64]],
    br_reach: float,
    opp_reach: float,
    infoset_values: dict,
) -> float:
    """Accumulate counterfactual values for each (infoset, action) pair.

    CRITICAL FIX: We need to track both the value sum AND the reach sum for proper normalization.
    When the same information set is reached from different worlds (chance outcomes),
    we accumulate values weighted by opponent reach, and track the total reach.

    This fixes the negative NashConv bug by ensuring proper normalization.
    """
    if state.is_terminal():
        returns = state.returns()
        return returns[best_responder]

    if state.is_chance_node():
        value = 0.0
        for action, probability in state.chance_outcomes():
            next_state = state.apply_action(action)
            value += probability * _accumulate_infoset_values(
                next_state, best_responder, opponent_strategy, br_reach, opp_reach, infoset_values
            )
        return value

    current_player = state.current_player()
    info_state = state.information_state_string()
    legal_actions = state.legal_actions()

    if current_player == best_responder:
        # Accumulate counterfactual values for each action at this information set
        if info_state not in infoset_values:
            # Store both value sum and reach sum for proper normalization
            infoset_values[info_state] = {
                "values": {},  # action -> accumulated value
                "reach": 0.0,  # total opponent reach to this info set
            }

        action_values_sum = 0.0
        for action in legal_actions:
            next_state = state.apply_action(action)
            action_value = _accumulate_infoset_values(
                next_state, best_responder, opponent_strategy, br_reach, opp_reach, infoset_values
            )

            # Accumulate opponent-reach-weighted value for this (infoset, action) pair
            if action not in infoset_values[info_state]["values"]:
                infoset_values[info_state]["values"][action] = 0.0
            infoset_values[info_state]["values"][action] += opp_reach * action_value
            action_values_sum += action_value

        # Track total opponent reach to this information set
        infoset_values[info_state]["reach"] += opp_reach

        # Return average value for this node (used in parent recursion)
        return action_values_sum / len(legal_actions)
    else:
        # Opponent follows their strategy
        if info_state in opponent_strategy:
            strategy = opponent_strategy[info_state]
        else:
            num_actions = len(legal_actions)
            strategy = np.ones(num_actions) / num_actions

        value = 0.0
        for i, action in enumerate(legal_actions):
            next_state = state.apply_action(action)
            value += strategy[i] * _accumulate_infoset_values(
                next_state,
                best_responder,
                opponent_strategy,
                br_reach,
                opp_reach * strategy[i],
                infoset_values,
            )
        return value


def _evaluate_strategy(
    state: GameState,
    player: int,
    player_strategy: dict[str, npt.NDArray[np.float64]],
    opponent_strategy: dict[str, npt.NDArray[np.float64]],
) -> float:
    """Evaluate a strategy profile."""
    if state.is_terminal():
        returns = state.returns()
        return returns[player]

    if state.is_chance_node():
        value = 0.0
        for action, probability in state.chance_outcomes():
            next_state = state.apply_action(action)
            value += probability * _evaluate_strategy(
                next_state, player, player_strategy, opponent_strategy
            )
        return value

    current_player = state.current_player()
    info_state = state.information_state_string()
    legal_actions = state.legal_actions()

    if current_player == player:
        strategy = player_strategy.get(info_state)
        if strategy is None:
            num_actions = len(legal_actions)
            strategy = np.ones(num_actions) / num_actions
    else:
        strategy = opponent_strategy.get(info_state)
        if strategy is None:
            num_actions = len(legal_actions)
            strategy = np.ones(num_actions) / num_actions

    value = 0.0
    for i, action in enumerate(legal_actions):
        next_state = state.apply_action(action)
        value += strategy[i] * _evaluate_strategy(
            next_state, player, player_strategy, opponent_strategy
        )
    return value


def compute_exploitability(
    initial_state: GameState,
    strategy: dict[str, npt.NDArray[np.float64]],
) -> float:
    """Compute exploitability of a strategy profile.

    Exploitability = sum of incentives to deviate for both players.
    At Nash equilibrium, this equals 0.

    Args:
        initial_state: Initial game state (before cards are dealt)
        strategy: Combined strategy profile for both players

    Returns:
        Exploitability value (0 = Nash, >0 = exploitable)
    """
    # Split combined strategy into per-player strategies
    player_strategies = [{}, {}]

    def split_strategies(state: GameState):
        """Traverse game tree to assign info states to players."""
        if state.is_terminal():
            return

        if state.is_chance_node():
            for action, _ in state.chance_outcomes():
                split_strategies(state.apply_action(action))
        else:
            current_player = state.current_player()
            info_state = state.information_state_string()

            if info_state in strategy:
                player_strategies[current_player][info_state] = strategy[info_state]

            for action in state.legal_actions():
                split_strategies(state.apply_action(action))

    split_strategies(initial_state)

    # Compute best response values for each player
    # Each player best-responds to the opponent's fixed strategy
    # Pass the player's own strategy as fallback for tie-breaking
    br_value_p0 = best_response_value(initial_state, 0, player_strategies[1], player_strategies[0])
    br_value_p1 = best_response_value(initial_state, 1, player_strategies[0], player_strategies[1])

    # Compute strategy values (both players follow the strategy)
    strategy_value_p0 = compute_strategy_value(initial_state, strategy, 0)
    strategy_value_p1 = compute_strategy_value(initial_state, strategy, 1)

    # Exploitability = sum of gains from deviating
    gain_p0 = br_value_p0 - strategy_value_p0
    gain_p1 = br_value_p1 - strategy_value_p1

    exploitability = gain_p0 + gain_p1

    return exploitability


def compute_strategy_value(
    state: GameState,
    strategy: dict[str, npt.NDArray[np.float64]],
    player: int,
) -> float:
    """Compute expected value when both players follow the given strategy.

    Args:
        state: Current game state
        strategy: Strategy profile for both players
        player: Player whose value we're computing (0 or 1)

    Returns:
        Expected value for the player
    """
    if state.is_terminal():
        returns = state.returns()
        return returns[player]

    if state.is_chance_node():
        expected_value = 0.0
        for action, probability in state.chance_outcomes():
            next_state = state.apply_action(action)
            value = compute_strategy_value(next_state, strategy, player)
            expected_value += probability * value
        return expected_value

    # Player node: use the strategy
    info_state = state.information_state_string()
    legal_actions = state.legal_actions()

    if info_state in strategy:
        player_strategy = strategy[info_state]
    else:
        num_actions = len(legal_actions)
        player_strategy = np.ones(num_actions, dtype=np.float64) / num_actions

    expected_value = 0.0
    for i, action in enumerate(legal_actions):
        next_state = state.apply_action(action)
        value = compute_strategy_value(next_state, strategy, player)
        expected_value += player_strategy[i] * value

    return expected_value


def compute_nash_conv(
    initial_state: GameState,
    strategy: dict[str, npt.NDArray[np.float64]],
) -> float:
    """Compute NashConv metric (same as exploitability).

    Args:
        initial_state: Initial game state
        strategy: Strategy profile

    Returns:
        NashConv value (0 = Nash equilibrium)
    """
    return compute_exploitability(initial_state, strategy)


def evaluate_strategy_profile(
    initial_state: GameState,
    strategy: dict[str, npt.NDArray[np.float64]],
) -> dict[str, float]:
    """Evaluate a strategy profile with detailed metrics.

    Args:
        initial_state: Initial game state
        strategy: Strategy profile

    Returns:
        Dictionary containing:
        - exploitability: Total exploitability
        - strategy_value_p0: P0's value when both follow strategy
        - strategy_value_p1: P1's value when both follow strategy
        - best_response_value_p0: P0's value when P0 best-responds
        - best_response_value_p1: P1's value when P1 best-responds
        - gain_p0: How much P0 gains by deviating
        - gain_p1: How much P1 gains by deviating
    """
    # Split strategies
    player_strategies = [{}, {}]

    def split_strategies(state: GameState):
        if state.is_terminal():
            return
        if state.is_chance_node():
            for action, _ in state.chance_outcomes():
                split_strategies(state.apply_action(action))
        else:
            current_player = state.current_player()
            info_state = state.information_state_string()
            if info_state in strategy:
                player_strategies[current_player][info_state] = strategy[info_state]
            for action in state.legal_actions():
                split_strategies(state.apply_action(action))

    split_strategies(initial_state)

    # Compute values
    v_p0 = compute_strategy_value(initial_state, strategy, 0)
    v_p1 = compute_strategy_value(initial_state, strategy, 1)
    br_p0 = best_response_value(initial_state, 0, player_strategies[1], player_strategies[0])
    br_p1 = best_response_value(initial_state, 1, player_strategies[0], player_strategies[1])

    gain_p0 = br_p0 - v_p0
    gain_p1 = br_p1 - v_p1
    exploitability = gain_p0 + gain_p1

    return {
        "exploitability": exploitability,
        "strategy_value_p0": v_p0,
        "strategy_value_p1": v_p1,
        "best_response_value_p0": br_p0,
        "best_response_value_p1": br_p1,
        "gain_p0": gain_p0,
        "gain_p1": gain_p1,
    }
